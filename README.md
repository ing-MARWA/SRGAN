# Super-Resolution GAN (SRGAN) - Anime Face Dataset
This repository contains the implementation of a Super-Resolution Generative Adversarial Network (SRGAN) designed to enhance the resolution of low-quality images. The specific dataset used in this project is the Anime Face Dataset.

The goal of this SRGAN model is to upsample low-resolution (LR) images to high-resolution (HR) while maintaining the quality and details as much as possible. This approach leverages deep learning, particularly GANs, to create realistic high-resolution images.
## Table of Contents
- [Project Overview](#ProjectOverview)
- [Architecture](#Architecture)
   - [Generator](#Generator)
   - [Discriminator](#Discriminator)
   - [VGG19](#VGG19)
- [Dependencies](#Dependencies)
- [Dataset](#Dataset)
- [Training](#Training)
- [Results](#Results)
- [How to Use](#HowtoUse)
- [References](#References)
# Project Overview
In this project, we implement a Super-Resolution GAN based on deep learning models, including a generator, discriminator, and a pre-trained VGG19 network for feature extraction. The generator aims to convert low-resolution images into high-resolution ones, while the discriminator's role is to distinguish between real HR images and those generated by the model.

The Anime Face Dataset is utilized to train the model, focusing on enhancing the visual quality of anime faces. The dataset contains clean, cropped, 256x256 pixel images of anime characters.
# Architecture
## Generator
The generator is a convolutional neural network (CNN) based on residual blocks. It takes a low-resolution image as input and upsamples it using sub-pixel convolution layers to generate a high-resolution output. The generator architecture consists of:

- **Convolutional Layers with ReLU Activations**
- **Residual Blocks** 
- **Subpixel Layers (for Upsampling)**
## Discriminator
The discriminator is also a CNN that classifies images as either "real" or "generated." It contains:

- **Convolutional layers with LeakyReLU activations**
- **Batch normalization layers to stabilize training**
- **A residual block to further improve performance**
  
## VGG19
A pre-trained VGG19 model is used to extract feature representations from both the real and generated images. This helps the generator focus not just on pixel-level accuracy but also on producing perceptually realistic images. The VGG19 model is loaded up to the 17th layer for feature extraction.
## Dependencies
Make sure you have the following libraries installed:

## Dependencies

Make sure you have the following libraries installed:

- `tensorflow==2.x`
- `opencv-python`
- `matplotlib`
- `numpy`
- `Pillow`
- `shutil`
- `os`

## Install them using:

```bash
pip install tensorflow opencv-python matplotlib numpy Pillow
```

## Dataset

The Anime Face Dataset is used for training. Images are resized to 256x256 (HR) for the training set and the low-resolution images are generated by downsampling the HR images to 64x64. 

You can download the dataset from Kaggle:

- [Anime Face Dataset]([link-to-dataset](https://www.kaggle.com/datasets/scribbless/another-anime-face-dataset)) 

## Training

### Hyperparameters

- Learning rate: 0.0002
- Initial training epochs (generator-only): 5
- Total training epochs: 20
- Batch size: 16
- Steps per epoch: 500

### Training Process

1. **Initial Generator Training:** Train the generator independently to minimize the pixel-wise mean square error (MSE) between generated images and real high-resolution images. 
2. **GAN Training:** After pre-training the generator, both the generator and discriminator are trained together in a GAN setup. The generator's loss consists of:
   - **MSE loss:** Pixel-wise loss.
   - **GAN loss:** Encourages the generator to produce more realistic images.
   - **VGG loss:** Based on the difference in feature representations between the real and generated images.

During training, generated images are saved at various epochs to visualize progress.

## Results

The model generates visually appealing high-resolution images from low-resolution inputs. Here are some sample results from the SRGAN:

[Image showcasing original, downsampled, and super-resolved images]

## How to Use

1. **Clone the repository:**

```bash
git clone https://github.com/ing-MARWA/SRGAN.git
cd srgan-anime-faces
```

2. **Download the dataset** and update the `PATH` in the notebook to point to the dataset folder.
3. **Run the training script** in the provided Jupyter notebook to train the model and visualize the results.

## References

- **SRGAN Paper:** [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)
- **Anime Face Dataset:**[ Anime Face Dataset on Kaggle](https://www.kaggle.com/datasets/scribbless/another-anime-face-dataset)
